{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Twilight Imperium PDF Processor\n",
        "## Step 1: Extract Text from PDFs for LangChain Assistant\n",
        "\n",
        "This notebook processes the Twilight Imperium PDFs and extracts text content that will be used to build a LangChain-powered rules assistant.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import fitz  # PyMuPDF for PDF processing\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "import re\n",
        "from typing import List, Dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Learn to Play PDF exists: True\n",
            "Rulebook PDF exists: True\n",
            "Output directory created: processed_rules\n"
          ]
        }
      ],
      "source": [
        "# Define paths to our PDF files\n",
        "dataset_path = Path(\"dataset\")\n",
        "learn_to_play_pdf = dataset_path / \"ti-k0289_learn_to_playcompressed.pdf\"\n",
        "rulebook_pdf = dataset_path / \"ti10_rulebook_web-good.pdf\"\n",
        "\n",
        "# Verify files exist\n",
        "print(f\"Learn to Play PDF exists: {learn_to_play_pdf.exists()}\")\n",
        "print(f\"Rulebook PDF exists: {rulebook_pdf.exists()}\")\n",
        "\n",
        "# Create output directory for processed text\n",
        "output_dir = Path(\"processed_rules\")\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "print(f\"Output directory created: {output_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_text_from_pdf(pdf_path: Path, source_name: str) -> Dict[str, any]:\n",
        "    \"\"\"\n",
        "    Extract text from a PDF file and return structured data\n",
        "    \n",
        "    Args:\n",
        "        pdf_path: Path to the PDF file\n",
        "        source_name: Name to identify the source (e.g., 'learn_to_play' or 'rulebook')\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary containing extracted text and metadata\n",
        "    \"\"\"\n",
        "    print(f\"\\nProcessing {source_name}...\")\n",
        "    \n",
        "    # Open the PDF\n",
        "    doc = fitz.open(pdf_path)\n",
        "    \n",
        "    # Get total pages before processing (needed before closing document)\n",
        "    total_pages = len(doc)\n",
        "    \n",
        "    # Extract text from each page\n",
        "    pages_text = []\n",
        "    full_text = \"\"\n",
        "    \n",
        "    for page_num in range(total_pages):\n",
        "        page = doc[page_num]\n",
        "        text = page.get_text()\n",
        "        \n",
        "        # Clean up the text - remove excessive whitespace\n",
        "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)  # Replace multiple newlines with double newline\n",
        "        text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with single space\n",
        "        text = text.strip()\n",
        "        \n",
        "        if text:  # Only add non-empty pages\n",
        "            pages_text.append({\n",
        "                'page_number': page_num + 1,\n",
        "                'text': text\n",
        "            })\n",
        "            full_text += f\"\\n\\n--- Page {page_num + 1} ---\\n\\n{text}\"\n",
        "    \n",
        "    doc.close()\n",
        "    \n",
        "    result = {\n",
        "        'source': source_name,\n",
        "        'total_pages': total_pages,\n",
        "        'pages_with_text': len(pages_text),\n",
        "        'full_text': full_text.strip(),\n",
        "        'pages': pages_text\n",
        "    }\n",
        "    \n",
        "    print(f\"Extracted text from {len(pages_text)} pages out of {total_pages} total pages\")\n",
        "    print(f\"Total characters extracted: {len(full_text)}\")\n",
        "    \n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing learn_to_play...\n",
            "Extracted text from 25 pages out of 25 total pages\n",
            "Total characters extracted: 100025\n"
          ]
        }
      ],
      "source": [
        "# Extract text from Learn to Play PDF\n",
        "learn_to_play_data = extract_text_from_pdf(learn_to_play_pdf, \"learn_to_play\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing rulebook...\n",
            "Extracted text from 16 pages out of 16 total pages\n",
            "Total characters extracted: 57697\n"
          ]
        }
      ],
      "source": [
        "# Extract text from Rulebook PDF\n",
        "rulebook_data = extract_text_from_pdf(rulebook_pdf, \"rulebook\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved learn_to_play data to:\n",
            "  JSON: processed_rules\\learn_to_play.json\n",
            "  Text: processed_rules\\learn_to_play.txt\n",
            "Saved rulebook data to:\n",
            "  JSON: processed_rules\\rulebook.json\n",
            "  Text: processed_rules\\rulebook.txt\n"
          ]
        }
      ],
      "source": [
        "# Save extracted text to files for later use\n",
        "def save_extracted_data(data: Dict, filename: str):\n",
        "    \"\"\"\n",
        "    Save extracted text data to both JSON and plain text files\n",
        "    \"\"\"\n",
        "    # Save as JSON for structured access\n",
        "    json_path = output_dir / f\"{filename}.json\"\n",
        "    with open(json_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "    \n",
        "    # Save as plain text for easy reading\n",
        "    txt_path = output_dir / f\"{filename}.txt\"\n",
        "    with open(txt_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(data['full_text'])\n",
        "    \n",
        "    print(f\"Saved {filename} data to:\")\n",
        "    print(f\"  JSON: {json_path}\")\n",
        "    print(f\"  Text: {txt_path}\")\n",
        "\n",
        "# Save both extracted datasets\n",
        "save_extracted_data(learn_to_play_data, \"learn_to_play\")\n",
        "save_extracted_data(rulebook_data, \"rulebook\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "EXTRACTION SUMMARY\n",
            "==================================================\n",
            "\n",
            "Learn to Play:\n",
            "  Total pages: 25\n",
            "  Pages with text: 25\n",
            "  Characters extracted: 100,023\n",
            "\n",
            "Rulebook:\n",
            "  Total pages: 16\n",
            "  Pages with text: 16\n",
            "  Characters extracted: 57,695\n",
            "\n",
            "Total characters from both PDFs: 157,718\n"
          ]
        }
      ],
      "source": [
        "# Display summary statistics\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"EXTRACTION SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(f\"\\nLearn to Play:\")\n",
        "print(f\"  Total pages: {learn_to_play_data['total_pages']}\")\n",
        "print(f\"  Pages with text: {learn_to_play_data['pages_with_text']}\")\n",
        "print(f\"  Characters extracted: {len(learn_to_play_data['full_text']):,}\")\n",
        "\n",
        "print(f\"\\nRulebook:\")\n",
        "print(f\"  Total pages: {rulebook_data['total_pages']}\")\n",
        "print(f\"  Pages with text: {rulebook_data['pages_with_text']}\")\n",
        "print(f\"  Characters extracted: {len(rulebook_data['full_text']):,}\")\n",
        "\n",
        "total_chars = len(learn_to_play_data['full_text']) + len(rulebook_data['full_text'])\n",
        "print(f\"\\nTotal characters from both PDFs: {total_chars:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "CONTENT PREVIEW\n",
            "==================================================\n",
            "\n",
            "🔹 Learn to Play (first 500 characters):\n",
            "----------------------------------------\n",
            "--- Page 1 ---\n",
            "\n",
            "®\n",
            "\n",
            "--- Page 2 ---\n",
            "\n",
            "2 My name is Mahthom Iq Seerva. I am the Winnaran keeper of the Custodian Chronicle, and I write this from the ancient Tower of Annals in old Mecatol City. Since inheriting the duties of the chronicle from my father, I have enjoyed the inspiring views of great buildings, ancient towers, and the bright lights of life that stretch into the distance. Yet, like the shadow at my feet, I can never escape or forget the lethally finite borders of this city. Less than a...\n",
            "\n",
            "🔹 Rulebook (first 500 characters):\n",
            "----------------------------------------\n",
            "--- Page 1 ---\n",
            "\n",
            "®\n",
            "\n",
            "--- Page 2 ---\n",
            "\n",
            "2 Teallian shook her head. “No, researcher. It’s not wasteful at all.” “What do you mean?” Hiari asked. The Barony officer gave her a small, superior smile. “It’s arrogant, Doctor. Pure arrogance to demonstrate their power by reshaping a planet to get what they want. And I approve.” ————————— Two weeks later, the shuttle soared across the expanse of the exposed seabed. Through her cabin viewport, Hiari could see rivers of sludge flowed through the wasteland, sl...\n"
          ]
        }
      ],
      "source": [
        "# Preview some of the extracted content\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"CONTENT PREVIEW\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(\"\\n🔹 Learn to Play (first 500 characters):\")\n",
        "print(\"-\" * 40)\n",
        "print(learn_to_play_data['full_text'][:500] + \"...\")\n",
        "\n",
        "print(\"\\n🔹 Rulebook (first 500 characters):\")\n",
        "print(\"-\" * 40)\n",
        "print(rulebook_data['full_text'][:500] + \"...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ PDF processing complete!\n",
            "📁 All extracted data saved to: processed_rules\n",
            "🚀 Ready for Step 2: Text chunking and embedding\n",
            "\n",
            "Combined data saved to: processed_rules\\combined_extracted_data.json\n"
          ]
        }
      ],
      "source": [
        "# Prepare data structure for next steps (chunking and embedding)\n",
        "combined_data = {\n",
        "    'learn_to_play': learn_to_play_data,\n",
        "    'rulebook': rulebook_data,\n",
        "    'processing_complete': True,\n",
        "    'next_steps': [\n",
        "        'Chunk the text using RecursiveCharacterTextSplitter',\n",
        "        'Generate embeddings for each chunk',\n",
        "        'Store in vector database (FAISS or Chroma)',\n",
        "        'Create LangChain tool for rule search',\n",
        "        'Build the chatbot agent'\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Save combined data for easy access in next notebook\n",
        "combined_path = output_dir / \"combined_extracted_data.json\"\n",
        "with open(combined_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(combined_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"\\n✅ PDF processing complete!\")\n",
        "print(f\"📁 All extracted data saved to: {output_dir}\")\n",
        "print(f\"🚀 Ready for Step 2: Text chunking and embedding\")\n",
        "print(f\"\\nCombined data saved to: {combined_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "twilight-imperium",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
