{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Twilight Imperium Text Chunker\n",
        "## Step 2: Chunk Text for Vector Embeddings\n",
        "\n",
        "This notebook takes the extracted PDF text and breaks it into smaller, manageable chunks that can be embedded and searched efficiently by our LangChain assistant.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Found processed data from Step 1\n",
            "📊 Loaded data contains:\n",
            "  - Learn to Play: 100,023 characters\n",
            "  - Rulebook: 57,695 characters\n"
          ]
        }
      ],
      "source": [
        "# Load the extracted data from Step 1\n",
        "processed_rules_dir = Path(\"processed_rules\")\n",
        "combined_data_path = processed_rules_dir / \"combined_extracted_data.json\"\n",
        "\n",
        "# Check if the processed data exists\n",
        "if not combined_data_path.exists():\n",
        "    print(\"❌ Error: Could not find processed data from Step 1.\")\n",
        "    print(\"Please run the pdf_processor.ipynb notebook first!\")\n",
        "else:\n",
        "    print(\"✅ Found processed data from Step 1\")\n",
        "    \n",
        "    # Load the data\n",
        "    with open(combined_data_path, 'r', encoding='utf-8') as f:\n",
        "        extracted_data = json.load(f)\n",
        "    \n",
        "    print(f\"📊 Loaded data contains:\")\n",
        "    print(f\"  - Learn to Play: {len(extracted_data['learn_to_play']['full_text']):,} characters\")\n",
        "    print(f\"  - Rulebook: {len(extracted_data['rulebook']['full_text']):,} characters\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔧 Text splitter configured:\n",
            "  - Chunk size: 800\n",
            "  - Chunk overlap: 100\n",
            "  - Separators: ['\\n\\n', '\\n', '. ']... (and more)\n"
          ]
        }
      ],
      "source": [
        "# Configure the text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Chunk size - optimal for embedding models and retrieval\n",
        "    chunk_size=800,\n",
        "    \n",
        "    # Overlap between chunks to maintain context\n",
        "    chunk_overlap=100,\n",
        "    \n",
        "    # Try to split on these separators in order (preserves structure)\n",
        "    separators=[\n",
        "        \"\\n\\n\",  # Double newlines (paragraphs)\n",
        "        \"\\n\",    # Single newlines\n",
        "        \". \",    # Sentences\n",
        "        \", \",    # Clauses\n",
        "        \" \",     # Words\n",
        "        \"\"       # Characters (last resort)\n",
        "    ],\n",
        "    \n",
        "    # Keep related text together\n",
        "    keep_separator=True\n",
        ")\n",
        "\n",
        "print(\"🔧 Text splitter configured:\")\n",
        "print(f\"  - Chunk size: {text_splitter._chunk_size}\")\n",
        "print(f\"  - Chunk overlap: {text_splitter._chunk_overlap}\")\n",
        "print(f\"  - Separators: {text_splitter._separators[:3]}... (and more)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_chunks_with_metadata(text: str, source: str, doc_type: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Split text into chunks and add metadata for better retrieval\n",
        "    \n",
        "    Args:\n",
        "        text: The full text to chunk\n",
        "        source: Source identifier ('learn_to_play' or 'rulebook')\n",
        "        doc_type: Human-readable document type\n",
        "        \n",
        "    Returns:\n",
        "        List of chunks with metadata\n",
        "    \"\"\"\n",
        "    print(f\"\\n📝 Processing {doc_type}...\")\n",
        "    \n",
        "    # Split the text into chunks\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    \n",
        "    # Create chunks with metadata\n",
        "    chunks_with_metadata = []\n",
        "    \n",
        "    for i, chunk in enumerate(chunks):\n",
        "        # Extract potential section headers (lines that are short and may be titles)\n",
        "        lines = chunk.split('\\n')\n",
        "        potential_header = None\n",
        "        \n",
        "        # Look for short lines that might be headers (under 100 chars, not mostly spaces)\n",
        "        for line in lines[:3]:  # Check first 3 lines\n",
        "            if line.strip() and len(line.strip()) < 100 and not line.strip().startswith('---'):\n",
        "                if len(line.strip().split()) <= 8:  # Likely a header if 8 words or fewer\n",
        "                    potential_header = line.strip()\n",
        "                    break\n",
        "        \n",
        "        # Create metadata for this chunk\n",
        "        metadata = {\n",
        "            'source': source,\n",
        "            'doc_type': doc_type,\n",
        "            'chunk_id': f\"{source}_chunk_{i:03d}\",\n",
        "            'chunk_index': i,\n",
        "            'total_chunks': len(chunks),\n",
        "            'char_count': len(chunk),\n",
        "            'word_count': len(chunk.split()),\n",
        "        }\n",
        "        \n",
        "        # Add section header if found\n",
        "        if potential_header:\n",
        "            metadata['section'] = potential_header\n",
        "        \n",
        "        # Store the chunk with its metadata\n",
        "        chunks_with_metadata.append({\n",
        "            'content': chunk.strip(),\n",
        "            'metadata': metadata\n",
        "        })\n",
        "    \n",
        "    print(f\"  ✅ Created {len(chunks_with_metadata)} chunks\")\n",
        "    print(f\"  📊 Average chunk size: {sum(len(c['content']) for c in chunks_with_metadata) // len(chunks_with_metadata)} characters\")\n",
        "    \n",
        "    return chunks_with_metadata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📝 Processing Learn to Play Guide...\n",
            "  ✅ Created 182 chunks\n",
            "  📊 Average chunk size: 581 characters\n"
          ]
        }
      ],
      "source": [
        "# Process Learn to Play document\n",
        "learn_to_play_chunks = create_chunks_with_metadata(\n",
        "    text=extracted_data['learn_to_play']['full_text'],\n",
        "    source='learn_to_play',\n",
        "    doc_type='Learn to Play Guide'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📝 Processing Official Rulebook...\n",
            "  ✅ Created 104 chunks\n",
            "  📊 Average chunk size: 585 characters\n"
          ]
        }
      ],
      "source": [
        "# Process Rulebook document\n",
        "rulebook_chunks = create_chunks_with_metadata(\n",
        "    text=extracted_data['rulebook']['full_text'],\n",
        "    source='rulebook',\n",
        "    doc_type='Official Rulebook'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "CHUNKING SUMMARY\n",
            "============================================================\n",
            "\n",
            "📊 Overall Statistics:\n",
            "  Total chunks created: 286\n",
            "  Learn to Play chunks: 182\n",
            "  Rulebook chunks: 104\n",
            "\n",
            "📏 Chunk Size Distribution:\n",
            "  Average size: 583 characters\n",
            "  Minimum size: 14 characters\n",
            "  Maximum size: 800 characters\n",
            "\n",
            "🏷️  Metadata Statistics:\n",
            "  Chunks with section headers: 4\n",
            "  Percentage with headers: 1.4%\n"
          ]
        }
      ],
      "source": [
        "# Combine all chunks\n",
        "all_chunks = learn_to_play_chunks + rulebook_chunks\n",
        "\n",
        "# Display comprehensive statistics\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CHUNKING SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\n📊 Overall Statistics:\")\n",
        "print(f\"  Total chunks created: {len(all_chunks)}\")\n",
        "print(f\"  Learn to Play chunks: {len(learn_to_play_chunks)}\")\n",
        "print(f\"  Rulebook chunks: {len(rulebook_chunks)}\")\n",
        "\n",
        "# Calculate size statistics\n",
        "chunk_sizes = [len(chunk['content']) for chunk in all_chunks]\n",
        "avg_size = sum(chunk_sizes) // len(chunk_sizes)\n",
        "min_size = min(chunk_sizes)\n",
        "max_size = max(chunk_sizes)\n",
        "\n",
        "print(f\"\\n📏 Chunk Size Distribution:\")\n",
        "print(f\"  Average size: {avg_size} characters\")\n",
        "print(f\"  Minimum size: {min_size} characters\")\n",
        "print(f\"  Maximum size: {max_size} characters\")\n",
        "\n",
        "# Count chunks with sections\n",
        "chunks_with_sections = sum(1 for chunk in all_chunks if 'section' in chunk['metadata'])\n",
        "print(f\"\\n🏷️  Metadata Statistics:\")\n",
        "print(f\"  Chunks with section headers: {chunks_with_sections}\")\n",
        "print(f\"  Percentage with headers: {chunks_with_sections/len(all_chunks)*100:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "SAMPLE CHUNKS PREVIEW\n",
            "============================================================\n",
            "\n",
            "🔹 Learn to Play - Sample Chunk:\n",
            "   Chunk ID: learn_to_play_chunk_000\n",
            "   Size: 33 characters\n",
            "   Section: ®\n",
            "   Content Preview:\n",
            "----------------------------------------\n",
            "--- Page 1 ---\n",
            "\n",
            "®\n",
            "\n",
            "--- Page 2 ---\n",
            "\n",
            "🔹 Rulebook - Sample Chunk:\n",
            "   Chunk ID: rulebook_chunk_000\n",
            "   Size: 33 characters\n",
            "   Section: ®\n",
            "   Content Preview:\n",
            "----------------------------------------\n",
            "--- Page 1 ---\n",
            "\n",
            "®\n",
            "\n",
            "--- Page 2 ---\n"
          ]
        }
      ],
      "source": [
        "# Preview sample chunks\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SAMPLE CHUNKS PREVIEW\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Show a few sample chunks from each document\n",
        "sample_learn_to_play = learn_to_play_chunks[0] if learn_to_play_chunks else None\n",
        "sample_rulebook = rulebook_chunks[0] if rulebook_chunks else None\n",
        "\n",
        "if sample_learn_to_play:\n",
        "    print(f\"\\n🔹 Learn to Play - Sample Chunk:\")\n",
        "    print(f\"   Chunk ID: {sample_learn_to_play['metadata']['chunk_id']}\")\n",
        "    print(f\"   Size: {sample_learn_to_play['metadata']['char_count']} characters\")\n",
        "    if 'section' in sample_learn_to_play['metadata']:\n",
        "        print(f\"   Section: {sample_learn_to_play['metadata']['section']}\")\n",
        "    print(f\"   Content Preview:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(sample_learn_to_play['content'][:300] + \"...\" if len(sample_learn_to_play['content']) > 300 else sample_learn_to_play['content'])\n",
        "\n",
        "if sample_rulebook:\n",
        "    print(f\"\\n🔹 Rulebook - Sample Chunk:\")\n",
        "    print(f\"   Chunk ID: {sample_rulebook['metadata']['chunk_id']}\")\n",
        "    print(f\"   Size: {sample_rulebook['metadata']['char_count']} characters\")\n",
        "    if 'section' in sample_rulebook['metadata']:\n",
        "        print(f\"   Section: {sample_rulebook['metadata']['section']}\")\n",
        "    print(f\"   Content Preview:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(sample_rulebook['content'][:300] + \"...\" if len(sample_rulebook['content']) > 300 else sample_rulebook['content'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Text chunking complete!\n",
            "📁 Chunked data saved to: processed_rules\\chunked_data.json\n",
            "🚀 Ready for Step 3: Generate embeddings\n",
            "\n",
            "📋 Summary:\n",
            "  - Total chunks ready for embedding: 286\n",
            "  - Average chunk size: 583 characters\n",
            "  - Data saved with metadata for easy retrieval\n"
          ]
        }
      ],
      "source": [
        "# Save the chunked data for the next step (embeddings)\n",
        "chunked_data = {\n",
        "    'all_chunks': all_chunks,\n",
        "    'learn_to_play_chunks': learn_to_play_chunks,\n",
        "    'rulebook_chunks': rulebook_chunks,\n",
        "    'chunking_config': {\n",
        "        'chunk_size': text_splitter._chunk_size,\n",
        "        'chunk_overlap': text_splitter._chunk_overlap,\n",
        "        'total_chunks': len(all_chunks)\n",
        "    },\n",
        "    'statistics': {\n",
        "        'total_chunks': len(all_chunks),\n",
        "        'learn_to_play_chunks': len(learn_to_play_chunks),\n",
        "        'rulebook_chunks': len(rulebook_chunks),\n",
        "        'avg_chunk_size': avg_size,\n",
        "        'min_chunk_size': min_size,\n",
        "        'max_chunk_size': max_size,\n",
        "        'chunks_with_sections': chunks_with_sections\n",
        "    },\n",
        "    'processing_complete': True,\n",
        "    'next_steps': [\n",
        "        'Generate embeddings for each chunk using OpenAI or local model',\n",
        "        'Store chunks and embeddings in FAISS or Chroma vector database',\n",
        "        'Create LangChain retrieval tool',\n",
        "        'Build the chatbot agent',\n",
        "        'Test with sample queries'\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Save to file\n",
        "chunked_data_path = processed_rules_dir / \"chunked_data.json\"\n",
        "with open(chunked_data_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(chunked_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"\\n✅ Text chunking complete!\")\n",
        "print(f\"📁 Chunked data saved to: {chunked_data_path}\")\n",
        "print(f\"🚀 Ready for Step 3: Generate embeddings\")\n",
        "print(f\"\\n📋 Summary:\")\n",
        "print(f\"  - Total chunks ready for embedding: {len(all_chunks)}\")\n",
        "print(f\"  - Average chunk size: {avg_size} characters\")\n",
        "print(f\"  - Data saved with metadata for easy retrieval\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "twilight-imperium",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
