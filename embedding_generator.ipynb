{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Twilight Imperium Embedding Generator\n",
        "## Step 3: Generate Embeddings and Create Vector Database\n",
        "\n",
        "This notebook creates vector embeddings for all text chunks using OpenAI's text-embedding-3-small model and stores them in a FAISS vector database for fast similarity search.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 🔑 OpenAI API Setup Instructions\n",
        "\n",
        "**Before running this notebook, you need to set up your OpenAI API key:**\n",
        "\n",
        "### Step 1: Get OpenAI API Key\n",
        "1. Go to [OpenAI Platform](https://platform.openai.com/)\n",
        "2. Sign up or log in to your account\n",
        "3. Navigate to **API Keys** section\n",
        "4. Click **\"Create new secret key\"**\n",
        "5. Copy the key (starts with `sk-...`)\n",
        "\n",
        "### Step 2: Set Environment Variable\n",
        "**Windows (Anaconda Prompt):**\n",
        "```bash\n",
        "setx OPENAI_API_KEY \"your-api-key-here\"\n",
        "```\n",
        "\n",
        "**Alternative: Create .env file**\n",
        "1. Create a file named `.env` in your project root directory\n",
        "2. Add this line: `OPENAI_API_KEY=your-api-key-here`\n",
        "3. Save the file\n",
        "\n",
        "### Step 3: Verify Setup\n",
        "Run the cells below to verify your API key is working.\n",
        "\n",
        "**💰 Cost Estimate:** ~$0.01-0.02 for 286 chunks with text-embedding-3-small\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ All libraries imported successfully\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# LangChain imports\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Load environment variables (for .env file)\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "print(\"✅ All libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "API key found: True\n",
            "Key starts with: sk-proj-gU0tE3P-TebB...\n"
          ]
        }
      ],
      "source": [
        "# Force reload environment variables and check\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(override=True)  # This will load from .env file\n",
        "\n",
        "import os\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "print(f\"API key found: {api_key is not None}\")\n",
        "if api_key:\n",
        "    print(f\"Key starts with: {api_key[:20]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ OpenAI API key found and appears valid\n",
            "Key starts with: sk-proj-gU0tE3P-TebB...\n",
            "✅ OpenAI Embeddings model initialized successfully\n",
            "Model: text-embedding-3-small\n"
          ]
        }
      ],
      "source": [
        "# Verify OpenAI API key is set\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "if not api_key:\n",
        "    print(\"❌ OpenAI API key not found!\")\n",
        "    print(\"Please set your OPENAI_API_KEY environment variable or create a .env file\")\n",
        "    print(\"See the instructions above for how to do this.\")\n",
        "elif api_key.startswith(\"sk-\"):\n",
        "    print(\"✅ OpenAI API key found and appears valid\")\n",
        "    print(f\"Key starts with: {api_key[:20]}...\")\n",
        "else:\n",
        "    print(\"⚠️  API key found but format looks incorrect\")\n",
        "    print(\"OpenAI API keys should start with 'sk-'\")\n",
        "\n",
        "# Initialize the embedding model\n",
        "try:\n",
        "    embeddings_model = OpenAIEmbeddings(\n",
        "        model=\"text-embedding-3-small\",\n",
        "        openai_api_key=api_key\n",
        "    )\n",
        "    print(\"✅ OpenAI Embeddings model initialized successfully\")\n",
        "    print(f\"Model: {embeddings_model.model}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error initializing embeddings model: {e}\")\n",
        "    print(\"Please check your API key and internet connection\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Found chunked data from Step 2\n",
            "📊 Loaded 286 chunks:\n",
            "  - Learn to Play: 182 chunks\n",
            "  - Rulebook: 104 chunks\n",
            "  - Average chunk size: 583 characters\n"
          ]
        }
      ],
      "source": [
        "# Load the chunked data from Step 2\n",
        "processed_rules_dir = Path(\"processed_rules\")\n",
        "chunked_data_path = processed_rules_dir / \"chunked_data.json\"\n",
        "\n",
        "# Check if the chunked data exists\n",
        "if not chunked_data_path.exists():\n",
        "    print(\"❌ Error: Could not find chunked data from Step 2.\")\n",
        "    print(\"Please run the text_chunker.ipynb notebook first!\")\n",
        "else:\n",
        "    print(\"✅ Found chunked data from Step 2\")\n",
        "    \n",
        "    # Load the data\n",
        "    with open(chunked_data_path, 'r', encoding='utf-8') as f:\n",
        "        chunked_data = json.load(f)\n",
        "    \n",
        "    # Extract the chunks\n",
        "    all_chunks = chunked_data['all_chunks']\n",
        "    \n",
        "    print(f\"📊 Loaded {len(all_chunks)} chunks:\")\n",
        "    print(f\"  - Learn to Play: {chunked_data['statistics']['learn_to_play_chunks']} chunks\")\n",
        "    print(f\"  - Rulebook: {chunked_data['statistics']['rulebook_chunks']} chunks\")\n",
        "    print(f\"  - Average chunk size: {chunked_data['statistics']['avg_chunk_size']} characters\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📄 Converting chunks to LangChain Documents...\n",
            "✅ Created 286 Document objects\n",
            "📋 Sample metadata keys: ['source', 'doc_type', 'chunk_id', 'chunk_index', 'total_chunks', 'char_count', 'word_count', 'section']\n",
            "\n",
            "🔍 Sample Document:\n",
            "  Content length: 33 characters\n",
            "  Source: learn_to_play\n",
            "  Chunk ID: learn_to_play_chunk_000\n",
            "  Section: ®\n",
            "  Content preview: --- Page 1 ---\n",
            "\n",
            "®\n",
            "\n",
            "--- Page 2 ---...\n"
          ]
        }
      ],
      "source": [
        "# Convert chunks to LangChain Documents\n",
        "def create_langchain_documents(chunks: List[Dict[str, Any]]) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Convert our chunks with metadata to LangChain Document objects\n",
        "    \"\"\"\n",
        "    documents = []\n",
        "    \n",
        "    for chunk in chunks:\n",
        "        # Create a Document with the content and metadata\n",
        "        doc = Document(\n",
        "            page_content=chunk['content'],\n",
        "            metadata=chunk['metadata']\n",
        "        )\n",
        "        documents.append(doc)\n",
        "    \n",
        "    return documents\n",
        "\n",
        "# Create Document objects from our chunks\n",
        "print(\"📄 Converting chunks to LangChain Documents...\")\n",
        "documents = create_langchain_documents(all_chunks)\n",
        "\n",
        "print(f\"✅ Created {len(documents)} Document objects\")\n",
        "print(f\"📋 Sample metadata keys: {list(documents[0].metadata.keys())}\")\n",
        "\n",
        "# Preview a sample document\n",
        "sample_doc = documents[0]\n",
        "print(f\"\\n🔍 Sample Document:\")\n",
        "print(f\"  Content length: {len(sample_doc.page_content)} characters\")\n",
        "print(f\"  Source: {sample_doc.metadata['source']}\")\n",
        "print(f\"  Chunk ID: {sample_doc.metadata['chunk_id']}\")\n",
        "if 'section' in sample_doc.metadata:\n",
        "    print(f\"  Section: {sample_doc.metadata['section']}\")\n",
        "print(f\"  Content preview: {sample_doc.page_content[:200]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔥 Starting embedding generation...\n",
            "📊 Processing 286 documents\n",
            "⏳ This may take a few minutes depending on your internet connection...\n",
            "✅ Embeddings generated successfully!\n",
            "📈 Vector store created with 286 vectors\n",
            "🔢 Embedding dimension: 1536\n"
          ]
        }
      ],
      "source": [
        "# Generate embeddings and create FAISS vector store\n",
        "print(\"🔥 Starting embedding generation...\")\n",
        "print(f\"📊 Processing {len(documents)} documents\")\n",
        "print(\"⏳ This may take a few minutes depending on your internet connection...\")\n",
        "\n",
        "try:\n",
        "    # Create FAISS vector store from documents\n",
        "    # This will automatically generate embeddings for each document\n",
        "    vector_store = FAISS.from_documents(\n",
        "        documents=documents,\n",
        "        embedding=embeddings_model\n",
        "    )\n",
        "    \n",
        "    print(\"✅ Embeddings generated successfully!\")\n",
        "    print(f\"📈 Vector store created with {vector_store.index.ntotal} vectors\")\n",
        "    \n",
        "    # Get embedding dimension\n",
        "    # Test with a small text to get the dimension\n",
        "    test_embedding = embeddings_model.embed_query(\"test\")\n",
        "    embedding_dimension = len(test_embedding)\n",
        "    print(f\"🔢 Embedding dimension: {embedding_dimension}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Error generating embeddings: {e}\")\n",
        "    print(\"Please check your API key, internet connection, and API usage limits\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧪 Testing vector store with sample queries...\n",
            "\n",
            "🔍 Sample Query Results:\n",
            "==================================================\n",
            "\n",
            "🔸 Query 1: 'How do I move ships in combat?'\n",
            "   Found 3 similar documents:\n",
            "   📄 Result 1:\n",
            "      Source: learn_to_play\n",
            "      Chunk ID: learn_to_play_chunk_110\n",
            "      Preview: 13 2. MOVEMENT During the movement step of a tactical action, the active player may choose to move some of his units into the active system. Each ship...\n",
            "\n",
            "   📄 Result 2:\n",
            "      Source: learn_to_play\n",
            "      Chunk ID: learn_to_play_chunk_117\n",
            "      Preview: . 3. SPACE COMBAT If multiple players have ships in the active system, they must resolve a space combat in that system. During combat, the active play...\n",
            "\n",
            "   📄 Result 3:\n",
            "      Source: learn_to_play\n",
            "      Chunk ID: learn_to_play_chunk_118\n",
            "      Preview: . iii. MAKE COMBAT ROLLS: Each player rolls one die for each ship he has in the active system. If the result of a unit’s die roll is equal to or great...\n",
            "\n",
            "\n",
            "🔸 Query 2: 'What happens when I activate a system?'\n",
            "   Found 3 similar documents:\n",
            "   📄 Result 1:\n",
            "      Source: learn_to_play\n",
            "      Chunk ID: learn_to_play_chunk_113\n",
            "      Preview: . A player also cannot activate a system if he does not have any command tokens in his tactic pool. MOVEMENT EXAMPLE The Xxcha player activates the We...\n",
            "\n",
            "   📄 Result 2:\n",
            "      Source: learn_to_play\n",
            "      Chunk ID: learn_to_play_chunk_112\n",
            "      Preview: . THE TACTICAL ACTION This section contains detailed rules for performing a tactical action. To perform a tactical action, players follow these steps ...\n",
            "\n",
            "   📄 Result 3:\n",
            "      Source: learn_to_play\n",
            "      Chunk ID: learn_to_play_chunk_081\n",
            "      Preview: 9 To understand a tactical action, players need to understand many other game concepts, such as movement and combat. For this reason, the steps of a t...\n",
            "\n",
            "✅ Vector store testing complete!\n"
          ]
        }
      ],
      "source": [
        "# Test the vector store with a sample query\n",
        "print(\"🧪 Testing vector store with sample queries...\")\n",
        "\n",
        "test_queries = [\n",
        "    \"How do I move ships in combat?\",\n",
        "    \"What happens when I activate a system?\",\n",
        "    \"How do strategy cards work?\",\n",
        "    \"What are the victory conditions?\",\n",
        "    \"How does ground combat work?\"\n",
        "]\n",
        "\n",
        "print(\"\\n🔍 Sample Query Results:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for i, query in enumerate(test_queries[:2]):  # Test first 2 queries\n",
        "    print(f\"\\n🔸 Query {i+1}: '{query}'\")\n",
        "    \n",
        "    try:\n",
        "        # Perform similarity search\n",
        "        similar_docs = vector_store.similarity_search(\n",
        "            query=query,\n",
        "            k=3  # Get top 3 most similar chunks\n",
        "        )\n",
        "        \n",
        "        print(f\"   Found {len(similar_docs)} similar documents:\")\n",
        "        \n",
        "        for j, doc in enumerate(similar_docs):\n",
        "            print(f\"   📄 Result {j+1}:\")\n",
        "            print(f\"      Source: {doc.metadata['source']}\")\n",
        "            print(f\"      Chunk ID: {doc.metadata['chunk_id']}\")\n",
        "            if 'section' in doc.metadata:\n",
        "                print(f\"      Section: {doc.metadata['section']}\")\n",
        "            print(f\"      Preview: {doc.page_content[:150]}...\")\n",
        "            print()\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"   ❌ Error testing query: {e}\")\n",
        "\n",
        "print(\"✅ Vector store testing complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "💾 Saving vector store to: processed_rules\\vector_store\n",
            "✅ Vector store saved successfully!\n",
            "📁 Saved files in: processed_rules\\vector_store\n",
            "📋 Created files:\n",
            "   - index.faiss\n",
            "   - index.pkl\n",
            "⚙️  Configuration saved to: processed_rules\\embedding_config.json\n",
            "\n",
            "🎉 Step 3 Complete!\n",
            "📊 Summary:\n",
            "  - Generated embeddings for 286 text chunks\n",
            "  - Created FAISS vector store with 1536-dimensional vectors\n",
            "  - Saved vector store locally for fast loading\n",
            "  - Ready for Step 4: Create LangChain search tool\n",
            "\n",
            "🚀 Next step: Build the LangChain agent and chatbot interface!\n"
          ]
        }
      ],
      "source": [
        "# Save vector store to disk for later use\n",
        "vector_store_dir = processed_rules_dir / \"vector_store\"\n",
        "\n",
        "print(f\"💾 Saving vector store to: {vector_store_dir}\")\n",
        "\n",
        "try:\n",
        "    # Save the FAISS vector store\n",
        "    vector_store.save_local(str(vector_store_dir))\n",
        "    \n",
        "    print(\"✅ Vector store saved successfully!\")\n",
        "    print(f\"📁 Saved files in: {vector_store_dir}\")\n",
        "    \n",
        "    # List the created files\n",
        "    if vector_store_dir.exists():\n",
        "        files = list(vector_store_dir.glob(\"*\"))\n",
        "        print(f\"📋 Created files:\")\n",
        "        for file in files:\n",
        "            print(f\"   - {file.name}\")\n",
        "    \n",
        "    # Save embedding configuration for easy reloading\n",
        "    embedding_config = {\n",
        "        'model_name': 'text-embedding-3-small',\n",
        "        'embedding_dimension': embedding_dimension,\n",
        "        'total_vectors': vector_store.index.ntotal,\n",
        "        'total_documents': len(documents),\n",
        "        'vector_store_path': str(vector_store_dir),\n",
        "        'created_from_chunks': len(all_chunks),\n",
        "        'sources': {\n",
        "            'learn_to_play_chunks': chunked_data['statistics']['learn_to_play_chunks'],\n",
        "            'rulebook_chunks': chunked_data['statistics']['rulebook_chunks']\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Save config file\n",
        "    config_path = processed_rules_dir / \"embedding_config.json\"\n",
        "    with open(config_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(embedding_config, f, indent=2, ensure_ascii=False)\n",
        "    \n",
        "    print(f\"⚙️  Configuration saved to: {config_path}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Error saving vector store: {e}\")\n",
        "\n",
        "print(f\"\\n🎉 Step 3 Complete!\")\n",
        "print(f\"📊 Summary:\")\n",
        "print(f\"  - Generated embeddings for {len(documents)} text chunks\")\n",
        "print(f\"  - Created FAISS vector store with {embedding_dimension}-dimensional vectors\")\n",
        "print(f\"  - Saved vector store locally for fast loading\")\n",
        "print(f\"  - Ready for Step 4: Create LangChain search tool\")\n",
        "print(f\"\\n🚀 Next step: Build the LangChain agent and chatbot interface!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "twilight-imperium",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
